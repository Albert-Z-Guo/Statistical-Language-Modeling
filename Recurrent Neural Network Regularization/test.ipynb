{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Py3 = sys.version_info[0] == 3\n",
    "\n",
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        if Py3:\n",
    "            return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "        else:\n",
    "            return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "    word_to_id = {}\n",
    "    word_to_id['UNK'] = 0\n",
    "    word_count_sorted = sorted(collections.Counter(data).items(), key=lambda item: item[1])\n",
    "    for item in word_count_sorted:\n",
    "        if item[1] > 3: # if word frequency > 3\n",
    "            word_to_id[item[0]] = len(word_to_id) # index by dictionary length, starting from 1\n",
    "        else:\n",
    "            word_to_id['UNK'] += 1\n",
    "    return word_to_id\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id.get(word, 0) for word in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'simple-examples/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27247\n"
     ]
    }
   ],
   "source": [
    "word_to_id = _build_vocab(os.path.join(data_path, \"wiki.train.txt\"))\n",
    "print(len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = dict(zip(word_to_id.values(), word_to_id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Battlefield\n",
      "Azure\n",
      "replayed\n",
      "unaltered\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6, 1):\n",
    "    print(id_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[27092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptb_raw_data(data_path):\n",
    "    train_path = os.path.join(data_path, \"wiki.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"wiki.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"wiki.test.txt\")\n",
    "\n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary = ptb_raw_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{25793, 26979, 26756, 26532, 24971, 23403, 25262, 25968, 26929, 26106, 27092, 26997, 26068, 25687, 25017, 26266, 26525, 26591}\n"
     ]
    }
   ],
   "source": [
    "def generate_tokens(data_path, token_class):\n",
    "    # create word-id dictionary\n",
    "    word_to_id = _build_vocab(os.path.join(data_path, \"wiki.train.txt\"))\n",
    "\n",
    "    # generate token ids\n",
    "    tens = [i for i in range(10, 100, 10)]\n",
    "    hundreds = [i for i in range(100, 1000, 100)]\n",
    "    rounds = set([word_to_id.get(str(i), 0) for i in tens + hundreds])\n",
    "    days = set([word_to_id.get(str(i), 0) for i in range(1, 32, 1)])\n",
    "    years = set([word_to_id.get(str(i), 0) for i in range(1000, 2021, 1)])\n",
    "    \n",
    "    if token_class == 'rounds':\n",
    "        return rounds\n",
    "    elif token_class == 'days':\n",
    "        return days\n",
    "    else:\n",
    "        return years\n",
    "\n",
    "rounds = generate_tokens(data_path, 'rounds')\n",
    "print(rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, batch_size, num_steps, tokens):\n",
    "    data_len = len(data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = np.reshape(data[0: batch_size * batch_len], [batch_size, batch_len])\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    print(epoch_size)\n",
    "    print(data.shape)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        positions = []\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            for k in range(num_steps):\n",
    "                if data[j][i + k] in tokens:\n",
    "                    positions.append((j, k, data[j][i + k]))\n",
    "\n",
    "        if i == batch_len - num_steps - 1:\n",
    "            i = 1\n",
    "        else:\n",
    "            i += 1\n",
    "            \n",
    "        yield positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator(train_data, 20, 20, rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5221\n",
      "(20, 104431)\n",
      "[(14, 10, 27092)]\n",
      "[(14, 9, 27092)]\n",
      "[(14, 8, 27092)]\n",
      "[(14, 7, 27092)]\n",
      "[(14, 6, 27092)]\n",
      "[(14, 5, 27092)]\n",
      "[(14, 4, 27092)]\n",
      "[(14, 3, 27092)]\n",
      "[(14, 2, 27092)]\n",
      "[(14, 1, 27092)]\n",
      "[(14, 0, 27092)]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    a = next(gen)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for k in range(1):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
